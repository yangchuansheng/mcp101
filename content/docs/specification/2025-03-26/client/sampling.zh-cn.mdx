---
title: 采样
---

<Callout type="info">
**协议修订版本**: 2025-03-26
</Callout>

模型上下文协议 (MCP) 提供了一种标准化的方式，供服务器通过客户端向语言模型请求 LLM 采样（也称为“补全”或“生成”）。此流程允许客户端保持对模型访问、选择和权限的控制，同时使服务器能够利用 AI 能力——无需服务器 API 密钥。服务器可以请求基于文本、音频或图像的交互，并可选择在提示中包含来自 MCP 服务器的上下文。

## 用户交互模型

MCP 中的采样允许服务器实现代理行为，通过在其他 MCP 服务器功能内部嵌套 LLM 调用来实现。

实现可以自由地通过任何适合其需求的界面模式来暴露采样功能——协议本身不强制要求任何特定的用户交互模型。

<Callout type="warn">
为了信任、安全和保障，**应该**始终有人工审核环节，并有权拒绝采样请求。

应用程序**应该**：

- 提供易于直观审查采样请求的用户界面
- 允许用户在发送前查看和编辑提示
- 在交付前呈现生成的响应以供审查
</Callout>

## 能力声明

支持采样的客户端**必须**在[初始化](/specification/2025-03-26/basic/lifecycle#initialization)期间声明 `sampling` 能力：

```json
{
  "capabilities": {
    "sampling": {}
  }
}
```

## 协议消息

### 创建消息

要请求语言模型生成，服务器发送一个 `sampling/createMessage` 请求：

**请求：**

```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "sampling/createMessage",
  "params": {
    "messages": [
      {
        "role": "user",
        "content": {
          "type": "text",
          "text": "法国的首都是哪里？"
        }
      }
    ],
    "modelPreferences": {
      "hints": [
        {
          "name": "claude-3-sonnet"
        }
      ],
      "intelligencePriority": 0.8,
      "speedPriority": 0.5
    },
    "systemPrompt": "你是一个乐于助人的助手。",
    "maxTokens": 100
  }
}
```

**响应：**

```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "result": {
    "role": "assistant",
    "content": {
      "type": "text",
      "text": "法国的首都是巴黎。"
    },
    "model": "claude-3-sonnet-20240307",
    "stopReason": "endTurn"
  }
}
```

## 消息流

<Mermaid chart={`
sequenceDiagram
    participant 服务器
    participant 客户端
    participant 用户
    participant LLM

    Note over 服务器,客户端: 服务器发起采样
    服务器->>客户端: sampling/createMessage

    Note over 客户端,用户: 人工审核环节
    客户端->>用户: 呈现请求以供批准
    用户-->>客户端: 审查并批准/修改

    Note over 客户端,LLM: 模型交互
    客户端->>LLM: 转发已批准的请求
    LLM-->>客户端: 返回生成结果

    Note over 客户端,用户: 响应审查
    客户端->>用户: 呈现响应以供批准
    用户-->>客户端: 审查并批准/修改

    Note over 服务器,客户端: 完成请求
    客户端-->>服务器: 返回已批准的响应
`} />

## 数据类型

### 消息

采样消息可以包含：

#### 文本内容

```json
{
  "type": "text",
  "text": "消息内容"
}
```

#### 图像内容

```json
{
  "type": "image",
  "data": "base64编码的图像数据",
  "mimeType": "image/jpeg"
}
```

#### 音频内容

```json
{
  "type": "audio",
  "data": "base64编码的音频数据",
  "mimeType": "audio/wav"
}
```

### 模型偏好

在 MCP 中选择模型需要仔细抽象，因为服务器和客户端可能使用不同的 AI 提供商，其模型产品也各不相同。服务器不能简单地通过名称请求特定模型，因为客户端可能无法访问该确切模型，或者可能更倾向于使用其他提供商的等效模型。

为了解决这个问题，MCP 实现了一个偏好系统，该系统结合了抽象的能力优先级和可选的模型提示：

#### 能力优先级

服务器通过三个归一化的优先级值（0-1）来表达其需求：

- `costPriority`：最小化成本有多重要？较高的值倾向于选择更便宜的模型。
- `speedPriority`：低延迟有多重要？较高的值倾向于选择更快的模型。
- `intelligencePriority`：高级能力有多重要？较高的值倾向于选择能力更强的模型。

#### 模型提示

虽然优先级有助于根据特性选择模型，但 `hints` 允许服务器建议特定的模型或模型系列：

- 提示被视为可以灵活匹配模型名称的子字符串
- 多个提示按偏好顺序进行评估
- 客户端**可以**将提示映射到来自不同提供商的等效模型
- 提示是建议性的——客户端做出最终的模型选择

例如：

```json
{
  "hints": [
    { "name": "claude-3-sonnet" }, // 偏好 Sonnet 级别的模型
    { "name": "claude" } // 退而求其次选择任何 Claude 模型
  ],
  "costPriority": 0.3, // 成本不太重要
  "speedPriority": 0.8, // 速度非常重要
  "intelligencePriority": 0.5 // 中等能力需求
}
```

客户端处理这些偏好，以从其可用选项中选择合适的模型。例如，如果客户端无法访问 Claude 模型但拥有 Gemini，它可能会根据相似的能力将 sonnet 提示映射到 `gemini-1.5-pro`。

## 错误处理

客户端**应该**为常见的失败情况返回错误：

错误示例：

```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "error": {
    "code": -1,
    "message": "用户拒绝了采样请求"
  }
}
```

## 安全考虑

1. 客户端**应该**实现用户批准控制
2. 双方**应该**验证消息内容
3. 客户端**应该**尊重模型偏好提示
4. 客户端**应该**实施速率限制
5. 双方**必须**适当地处理敏感数据
